{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_and_evaluate import predict_from_checkpoint_with_labels\n",
    "from training_utils import train_models_by_params\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transforms import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('../data/train_set.csv').values.flatten().tolist()\n",
    "val_set = pd.read_csv('../data/val_set.csv').values.flatten().tolist()\n",
    "\n",
    "ADC_dir = ['../ext_storage/mri_data/preprocessed/DTI_eddy_MD_wm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'basic_block_depth': 4,\n",
      " 'batch_size': 4,\n",
      " 'img_dirs': ['../ext_storage/mri_data/preprocessed/DTI_eddy_MD_wm'],\n",
      " 'learning_rate': 0.0003,\n",
      " 'total_epochs': 10,\n",
      " 'transforms': [Mirror()]}\n",
      "standard RUN version_2\n",
      "\n",
      "Loaded 291 images each with shape (1, 96, 112, 96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type           | Params\n",
      "-----------------------------------------------\n",
      "0 | net         | Basic3DCNN     | 425 K \n",
      "1 | train_acc   | BinaryAccuracy | 0     \n",
      "2 | train_auroc | BinaryAUROC    | 0     \n",
      "3 | val_acc     | BinaryAccuracy | 0     \n",
      "4 | val_auroc   | BinaryAUROC    | 0     \n",
      "5 | test_acc    | BinaryAccuracy | 0     \n",
      "6 | test_auroc  | BinaryAUROC    | 0     \n",
      "-----------------------------------------------\n",
      "425 K     Trainable params\n",
      "0         Non-trainable params\n",
      "425 K     Total params\n",
      "1.700     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 41 images each with shape (1, 96, 112, 96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96c070665a04fddb60e2a093fadb888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7ca4d2c1234dd1b67453ac6ee92b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/opt/conda/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c8529d260a453fbc5077092622097d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948f5fec57fb4f958284ce5ca89940cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb053496ea27421781b89eacd7d9afa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d5571527ca4cafadaacd5dd00d561a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee09325098e4bf38ba7a2ab0cbaae16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5d5f7ef8f64dea9de562eef91fb7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3c52f9564f4142aeae678a2e0b11d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a131617bab248b1bb47f8d6ad7bd40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a6f0185db54311ab9c235e6153ed09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751fb9fd4c454ad0b93d40357df1d117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained from 2023-10-14 09:32 to 2023-10-14 09:34\n",
      "Time elapsed (hh:mm:ss.ms) 0:01:50.089171\n",
      "\n",
      "####################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runs = [\n",
    "    { # Version 0\n",
    "    'img_dirs': ADC_dir,\n",
    "    'transforms': TRAIN_TRANSFORMS_MIRROR_PROB,\n",
    "    'basic_block_depth': 4,\n",
    "\n",
    "    'total_epochs': 10,\n",
    "    'batch_size': 4,\n",
    "    'learning_rate': 3e-4,\n",
    "},\n",
    "]\n",
    "\n",
    "train_models_by_params(\n",
    "    train_set, \n",
    "    runs,\n",
    "    validation_set = val_set\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 41 images each with shape (1, 96, 112, 96)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspace/HEAL_ML/code/testing_predict.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6865616c5f6d6c222c22637764223a22633a5c5c55736572735c5c6368726973227d/workspace/HEAL_ML/code/testing_predict.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Testing basic\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6865616c5f6d6c222c22637764223a22633a5c5c55736572735c5c6368726973227d/workspace/HEAL_ML/code/testing_predict.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m preds, test_labels \u001b[39m=\u001b[39m predict_from_checkpoint_with_labels(\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6865616c5f6d6c222c22637764223a22633a5c5c55736572735c5c6368726973227d/workspace/HEAL_ML/code/testing_predict.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     checkpoint_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m../ext_storage/saved_models_storage/lightning_logs/version_0/checkpoints/last.ckpt\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6865616c5f6d6c222c22637764223a22633a5c5c55736572735c5c6368726973227d/workspace/HEAL_ML/code/testing_predict.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     subject_list\u001b[39m=\u001b[39;49mval_set,\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6865616c5f6d6c222c22637764223a22633a5c5c55736572735c5c6368726973227d/workspace/HEAL_ML/code/testing_predict.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     img_dirs\u001b[39m=\u001b[39;49mADC_dir,\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6865616c5f6d6c222c22637764223a22633a5c5c55736572735c5c6368726973227d/workspace/HEAL_ML/code/testing_predict.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     net_architecture\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbasic\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6865616c5f6d6c222c22637764223a22633a5c5c55736572735c5c6368726973227d/workspace/HEAL_ML/code/testing_predict.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     basic_block_depth\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6865616c5f6d6c222c22637764223a22633a5c5c55736572735c5c6368726973227d/workspace/HEAL_ML/code/testing_predict.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     label_csv_columns \u001b[39m=\u001b[39;49m {\u001b[39m'\u001b[39;49m\u001b[39msubject_col\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mstudyid\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mlabel_col\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mprimary_all\u001b[39;49m\u001b[39m'\u001b[39;49m},\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6865616c5f6d6c222c22637764223a22633a5c5c55736572735c5c6368726973227d/workspace/HEAL_ML/code/testing_predict.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     additional_feature_cols \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6865616c5f6d6c222c22637764223a22633a5c5c55736572735c5c6368726973227d/workspace/HEAL_ML/code/testing_predict.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6865616c5f6d6c222c22637764223a22633a5c5c55736572735c5c6368726973227d/workspace/HEAL_ML/code/testing_predict.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(roc_auc_score(test_labels, preds))\n",
      "File \u001b[0;32m/workspace/HEAL_ML/code/train_and_evaluate.py:768\u001b[0m, in \u001b[0;36mpredict_from_checkpoint_with_labels\u001b[0;34m(checkpoint_path, subject_list, img_dirs, net_architecture, label_csv_columns, additional_feature_cols, efficientnet_model_name, densenet_class, dropout, resnet_class, basic_block_depth, ensemble_model_params, num_workers)\u001b[0m\n\u001b[1;32m    761\u001b[0m test_loader \u001b[39m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, num_workers\u001b[39m=\u001b[39mnum_workers, pin_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    763\u001b[0m \u001b[39m###############################################################################\u001b[39;00m\n\u001b[1;32m    764\u001b[0m \n\u001b[1;32m    765\u001b[0m \u001b[39m# Model training\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \n\u001b[1;32m    767\u001b[0m \u001b[39m# Get network from parameters; pretrained_layers will be none if not loading\u001b[39;00m\n\u001b[0;32m--> 768\u001b[0m net, _ \u001b[39m=\u001b[39m get_model_by_params(\n\u001b[1;32m    769\u001b[0m     img_dirs,\n\u001b[1;32m    770\u001b[0m     dropout,\n\u001b[1;32m    771\u001b[0m     net_architecture,\n\u001b[1;32m    772\u001b[0m     additional_feature_cols,\n\u001b[1;32m    773\u001b[0m     efficientnet_model_name,\n\u001b[1;32m    774\u001b[0m     resnet_class,\n\u001b[1;32m    775\u001b[0m     densenet_class,\n\u001b[1;32m    776\u001b[0m     basic_block_depth,\n\u001b[1;32m    777\u001b[0m     ensemble_model_params \u001b[39m=\u001b[39;49m ensemble_model_params,\n\u001b[1;32m    778\u001b[0m     pretrained_path \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    779\u001b[0m )\n\u001b[1;32m    781\u001b[0m model \u001b[39m=\u001b[39m LitHEAL(\n\u001b[1;32m    782\u001b[0m     net \u001b[39m=\u001b[39m net,\n\u001b[1;32m    783\u001b[0m     additional_inputs \u001b[39m=\u001b[39m \u001b[39mbool\u001b[39m(additional_feature_cols),\n\u001b[1;32m    784\u001b[0m     logistic_regression\u001b[39m=\u001b[39m \u001b[39mbool\u001b[39m(net_architecture \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlogistic_regression\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m    785\u001b[0m     lr \u001b[39m=\u001b[39m \u001b[39m3e-4\u001b[39m\n\u001b[1;32m    786\u001b[0m )\n\u001b[1;32m    788\u001b[0m \u001b[39m# Load checkpoint data\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/HEAL_ML/code/train_and_evaluate.py:59\u001b[0m, in \u001b[0;36mget_model_by_params\u001b[0;34m(img_dirs, dropout, net_architecture, additional_feature_cols, efficientnet_model_name, resnet_class, densenet_class, basic_block_depth, pretrained_path, ensemble_model_params)\u001b[0m\n\u001b[1;32m     52\u001b[0m     net \u001b[39m=\u001b[39m Basic3DCNN_with_additional_inputs(\n\u001b[1;32m     53\u001b[0m         num_additional_inputs \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(additional_feature_cols),\n\u001b[1;32m     54\u001b[0m         input_channels\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(img_dirs),\n\u001b[1;32m     55\u001b[0m         conv_block_depth \u001b[39m=\u001b[39m basic_block_depth,\n\u001b[1;32m     56\u001b[0m         dropout\u001b[39m=\u001b[39mdropout\n\u001b[1;32m     57\u001b[0m     )\n\u001b[1;32m     58\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     net \u001b[39m=\u001b[39m Basic3DCNN(\n\u001b[1;32m     60\u001b[0m         input_channels\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(img_dirs),\n\u001b[1;32m     61\u001b[0m         conv_block_depth \u001b[39m=\u001b[39;49m basic_block_depth,\n\u001b[1;32m     62\u001b[0m         dropout\u001b[39m=\u001b[39;49mdropout\n\u001b[1;32m     63\u001b[0m     )\n\u001b[1;32m     65\u001b[0m \u001b[39m# To do: add model loading\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m pretrained_path:\n",
      "File \u001b[0;32m/workspace/HEAL_ML/code/models.py:340\u001b[0m, in \u001b[0;36mBasic3DCNN.__init__\u001b[0;34m(self, input_channels, out_classes, conv_block_depth, conv_channel_multipier, dropout)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    318\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    319\u001b[0m     input_channels \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    323\u001b[0m     dropout \u001b[39m=\u001b[39m \u001b[39m0.3\u001b[39m\n\u001b[1;32m    324\u001b[0m ):  \n\u001b[1;32m    325\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[39m    ----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39m        Dropout percent after first linear layer\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 340\u001b[0m     \u001b[39msuper\u001b[39;49m(Basic3DCNN, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    342\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_block_list \u001b[39m=\u001b[39m _create_conv_block_set(\n\u001b[1;32m    343\u001b[0m         conv_block_depth, \n\u001b[1;32m    344\u001b[0m         input_channels, \n\u001b[1;32m    345\u001b[0m         conv_channel_multipier\n\u001b[1;32m    346\u001b[0m     )        \n\u001b[1;32m    348\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLazyLinear(\u001b[39m1024\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "# Testing basic\n",
    "\n",
    "preds, test_labels = predict_from_checkpoint_with_labels(\n",
    "    checkpoint_path='../ext_storage/saved_models_storage/lightning_logs/version_0/checkpoints/last.ckpt',\n",
    "    subject_list=val_set,\n",
    "    img_dirs=ADC_dir,\n",
    "    net_architecture='basic',\n",
    "    basic_block_depth=4,\n",
    "    label_csv_columns = {'subject_col': 'studyid', 'label_col': 'primary_all'},\n",
    "    additional_feature_cols = None,\n",
    ")\n",
    "\n",
    "print(roc_auc_score(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.0248], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.0824], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.1826], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.1122], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.0648], device='cuda:0', dtype=torch.float16),\n",
       " tensor([1.], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.5137], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.0970], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.4597], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.2520], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.1188], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.4968], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.6782], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.3745], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.7358], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.1844], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.9365], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.3806], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.9341], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.0983], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.3931], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.0381], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.0004], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.2123], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.3740], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.1890], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.8584], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.4905], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.8916], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.5317], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.0230], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.9995], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.0444], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.0059], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.0087], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.0368], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.8120], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.1436], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.2815], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.1622], device='cuda:0', dtype=torch.float16),\n",
       " tensor([0.0366], device='cuda:0', dtype=torch.float16)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets.densenet import DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 41 images each with shape (1, 96, 112, 96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f36e31019b4ea3ae25391749b97fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model run from 2023-10-13 10:46 to 2023-10-13 10:46\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:07.595535\n",
      "0.5049019607843137\n"
     ]
    }
   ],
   "source": [
    "# Testing dense\n",
    "\n",
    "preds, test_labels = predict_from_checkpoint_with_labels(\n",
    "    checkpoint_path='../ext_storage/saved_models_storage/lightning_logs_pre_9_18_23/version_281/checkpoints/last.ckpt',\n",
    "    subject_list=val_set,\n",
    "    img_dirs=['../ext_storage/mri_data/preprocessed/DTI_eddy_MD_wm'],\n",
    "    net_architecture='densenet',\n",
    "    densenet_class=DenseNet121,\n",
    "    label_csv_columns = {'subject_col': 'studyid', 'label_col': 'primary_all'},\n",
    "    additional_feature_cols = None,\n",
    ")\n",
    "\n",
    "print(roc_auc_score(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../ext_storage/saved_models_storage/lightning_logs/version_{}/checkpoints/last.ckpt'\n",
    "\n",
    "basic_ADC_pretrained_path = base_path.format(6)\n",
    "basic_T1_pretrained_path = base_path.format(21)\n",
    "basic_T2_pretrained_path = base_path.format(36)\n",
    "basic_trace_pretrained_path = base_path.format(51)\n",
    "\n",
    "densenet_ADC_pretrained_path = base_path.format(281)\n",
    "densenet_T1_pretrained_path = base_path.format(291)\n",
    "densenet_T2_pretrained_path = base_path.format(301)\n",
    "densenet_trace_pretrained_path = base_path.format(311)\n",
    "\n",
    "LR_pretrained_path = base_path.format(366)\n",
    "\n",
    "\n",
    "ensemble_model_params = {\n",
    "    'cnn_model_list': [\n",
    "        ## Basic\n",
    "        # ADC\n",
    "        {\n",
    "            'pretrained_path': basic_ADC_pretrained_path, # Pretrained paths are optional\n",
    "            'net_architecture': 'basic',\n",
    "            'basic_block_depth': 4,\n",
    "            'dropout': 0.3,\n",
    "        },\n",
    "        # T1\n",
    "        {\n",
    "            'pretrained_path': basic_T1_pretrained_path,\n",
    "            'net_architecture': 'basic',\n",
    "            'basic_block_depth': 4,\n",
    "            'dropout': 0.3,\n",
    "        },\n",
    "        # T2\n",
    "        {\n",
    "            'pretrained_path': basic_T2_pretrained_path,\n",
    "            'net_architecture': 'basic',\n",
    "            'basic_block_depth': 4,\n",
    "            'dropout': 0.3,\n",
    "        },\n",
    "        # T3\n",
    "        {\n",
    "            'pretrained_path': basic_trace_pretrained_path,\n",
    "            'net_architecture': 'basic',\n",
    "            'basic_block_depth': 4,\n",
    "            'dropout': 0.3,\n",
    "        },\n",
    "\n",
    "        ## Dense\n",
    "        {\n",
    "            'pretrained_path': densenet_ADC_pretrained_path, # Pretrained paths are optional\n",
    "            'net_architecture': 'densenet',\n",
    "            'densenet_class': DenseNet121,\n",
    "        },\n",
    "        # T1\n",
    "        {\n",
    "            'pretrained_path': densenet_T1_pretrained_path,\n",
    "            'net_architecture': 'densenet',\n",
    "            'densenet_class': DenseNet121,\n",
    "        },\n",
    "        # T2\n",
    "        {\n",
    "            'pretrained_path': densenet_T2_pretrained_path,\n",
    "            'net_architecture': 'densenet',\n",
    "            'densenet_class': DenseNet121,\n",
    "        },\n",
    "        # T3\n",
    "        {\n",
    "            'pretrained_path': densenet_trace_pretrained_path,\n",
    "            'net_architecture': 'densenet',\n",
    "            'densenet_class': DenseNet121,\n",
    "        },\n",
    "    ],\n",
    "    'cnn_image_index_list': [\n",
    "        0, 1, 2, 3, 0, 1, 2, 3\n",
    "    ], # For each corresponding model in above list\n",
    "\n",
    "    'lr_model_list': [\n",
    "        {\n",
    "            'pretrained_path': LR_pretrained_path,\n",
    "            'net_architecture': 'logistic_regression',\n",
    "        },\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing dense\n",
    "\n",
    "preds, test_labels = predict_from_checkpoint_with_labels(\n",
    "    checkpoint_path='../ext_storage/saved_models_storage/lightning_logs_pre_9_18_23/version_381/checkpoints/last.ckpt',\n",
    "    subject_list=val_set,\n",
    "    img_dirs=['../ext_storage/mri_data/preprocessed/DTI_eddy_MD_wm',\n",
    "              '../ext_storage/mri_data/preprocessed/T1_wm',\n",
    "              '../ext_storage/mri_data/preprocessed/T2_wm',\n",
    "              '../ext_storage/mri_data/preprocessed/DTI_eddy_trace_wm'],\n",
    "    net_architecture='ensemble',\n",
    "    ensemble_model_params=ensemble_model_params,\n",
    "    label_csv_columns = {'subject_col': 'studyid', 'label_col': 'primary_all'},\n",
    "    additional_feature_cols = ['sex', 'txtassign', 'inf_gestage_zscore', 'total_brain_injury_volume_zscore', 'inf_gestage_minmax', 'total_brain_injury_volume_minmax'],\n",
    ")\n",
    "\n",
    "print(roc_auc_score(test_labels, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
